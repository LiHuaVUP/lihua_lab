How to deal with categorical variables with more than 53 categories in R using Random Forest?
=================

Random Forest may be one of the most frequently used machine learning algorithms, and the application of [randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf) package in R makes it even more user - friendly. However, when you use this package, if you are not lucky enough, you may encounter an Error looks like:

```
Error in randomForest.default(m, y, ...) : 
  Can not handle categorical predictors with more than 53 categories.
```

It means that you use a categorical variable with more than 53 categories to fit the random forest model and it is not allowed in the nature of the algorithm coded in randomForest R package. 

But, the application categorical variable with more than 53 categories is actually not rare, especially in research of social science (countries, cities..... whatever). So how can we solve this problem if we really want to apply this model to our data.

First, I would like to talk about how R (and also all programming languages) deal with categories.

Let's assume we have a simple linear regression, using age (`x1`) and gender (`x2`) to predict height of people, the model would look like:

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;b_0&space;&plus;&space;b_1&space;x_1&space;&plus;&space;b_2&space;x_2&space;&plus;&space;\delta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;b_0&space;&plus;&space;b_1&space;x_1&space;&plus;&space;b_2&space;x_2&space;&plus;&space;\delta" title="y = b_0 + b_1 x_1 + b_2 x_2 + \delta" /></a>

where `b0`, `b1`, `b2` are fitted coefficients and <a href="https://www.codecogs.com/eqnedit.php?latex=\delta" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\delta" title="\delta" /></a> is residual.

When R tries to understand the gender of people, which is either female or male, it tends to treat it as a binary variable, which is either 0 (for female) or 1 (for male).

```
                 Gender
obs1 (Female)      0
obs2 (Male)        1
....................
```

However, in the real world, there are also other genders besides male and female, so normally, we get three kinds of genders, including female, male, and others. This feature makes gender a categorical variable with 3 categories and can not be covered by a single binary variable. Therefore, the programming language would read the data like:

```
                Female Male
obs1 (Female)      1    0
obs2 (Male)        0    1
obs3 (Others)      0    0
```
The programming language would generate two binary variables to identify the 3 categories, the first observation is female, so it is marked as 1 in the column Female and 0 in the column Male, opposite condition happens in the second observation, and the last observation gets 0 in both Female and Male, so its gender is "Others". The general rule is, if you get categorical variables with `m` categories, the programming language would treat it as `n` binary variables. So, if you get a categorical variable with more than 53 categories, you actually get more than 52 variables in your fitted model.

So my solution to solve the problme is, transferring the variable with 53+ variables to 53+ - 1 binary variables, and applying this refined dataset to feed the randomForest package in R.

Let's see an example. Here we get the Carbon Dioxide Emission in 73 cities of China in 2000, 2007, 2012, and 2017 (292 observations). Therefore, to fit the random forest, we get two predictors (Year and City) and one responsor (Carbon Dioxide Emission). All data are randomly generated from normal distribution so the model actually makes no sense, just for an example.

The data looks like:
```
city_features<-read.csv('city_features_short.csv',header=T)
city_features<-city_features[,-1]
head(city_features)

year      city Carbon.dioxide.emissions
2017  shanghai               -0.6747491
2017   nanjing               -0.1595591
2017      wuxi               -0.9915031
2017 changzhou                0.2415139
2017    suzhou               -1.2067791
2017   nantong                0.1250743
```

If you directly fit the model using this data, you would get an error like:
```
library(dplyr)
library(randomForest)
library(caret)
library(e1071)
set.seed(1234)
rf_final<-randomForest(Carbon.dioxide.emissions~.,data=city_features,
                        mtry=60,maxnodes=50,ntree=500,importance=TRUE)

Error in randomForest.default(m, y, ...) : 
  Can not handle categorical predictors with more than 53 categories.
```
That's reasonable, since we have 73 cities here...

Let's transfer the "city" variables to 73 binary variables, each of which indicate a particular city.
```
new_add<-data.frame(matrix(NA,nrow=292,ncol=73))
loc<-1
for(i in levels(city_features$city)){
  new_array<-city_features$city==i
  new_add[,loc]<-factor(new_array)
  loc<-loc+1
}
colnames(new_add)<-levels(city_features$city)
city_features<-cbind(city_features,(new_add))
city_features$city<-NULL
```
Now the data frame looks like:
```
head(city_features)
  year Carbon.dioxide.emissions anqing changde changsha changzhou chengdu chizhou
1 2017               -0.6747491  FALSE   FALSE    FALSE     FALSE   FALSE   FALSE
2 2017               -0.1595591  FALSE   FALSE    FALSE     FALSE   FALSE   FALSE
3 2017               -0.9915031  FALSE   FALSE    FALSE     FALSE   FALSE   FALSE
4 2017                0.2415139  FALSE   FALSE    FALSE      TRUE   FALSE   FALSE
5 2017               -1.2067791  FALSE   FALSE    FALSE     FALSE   FALSE   FALSE
6 2017                0.1250743  FALSE   FALSE    FALSE     FALSE   FALSE   FALSE
  chongqing chuzhou dazhou deyang ezhou fuzhou guangan hangzhou hefei hengyang
1     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
2     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
3     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
4     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
5     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
6     FALSE   FALSE  FALSE  FALSE FALSE  FALSE   FALSE    FALSE FALSE    FALSE
  huanggang huangshi huzhou jiaxing jingdezhen jingmen jingzhou jinhua jiujiang
1     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
2     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
3     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
4     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
5     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
6     FALSE    FALSE  FALSE   FALSE      FALSE   FALSE    FALSE  FALSE    FALSE
  leshan loudi luzhou maanshan meishan mianyang nanchang nanchong nanjing nantong
1  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE   FALSE   FALSE
2  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE    TRUE   FALSE
3  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE   FALSE   FALSE
4  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE   FALSE   FALSE
5  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE   FALSE   FALSE
6  FALSE FALSE  FALSE    FALSE   FALSE    FALSE    FALSE    FALSE   FALSE    TRUE
  neijiang ningbo pingxiang qianjiang shanghai shangrao shaoxing suining suzhou
1    FALSE  FALSE     FALSE     FALSE     TRUE    FALSE    FALSE   FALSE  FALSE
2    FALSE  FALSE     FALSE     FALSE    FALSE    FALSE    FALSE   FALSE  FALSE
3    FALSE  FALSE     FALSE     FALSE    FALSE    FALSE    FALSE   FALSE  FALSE
4    FALSE  FALSE     FALSE     FALSE    FALSE    FALSE    FALSE   FALSE  FALSE
5    FALSE  FALSE     FALSE     FALSE    FALSE    FALSE    FALSE   FALSE   TRUE
6    FALSE  FALSE     FALSE     FALSE    FALSE    FALSE    FALSE   FALSE  FALSE
  taizho taizhou tianmen tongling wenzhou wuhan  wuhu  wuxi xiangtan xiangyang
1  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE FALSE    FALSE     FALSE
2  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE FALSE    FALSE     FALSE
3  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE  TRUE    FALSE     FALSE
4  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE FALSE    FALSE     FALSE
5  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE FALSE    FALSE     FALSE
6  FALSE   FALSE   FALSE    FALSE   FALSE FALSE FALSE FALSE    FALSE     FALSE
  xianning xiantao xiaogan xinyu  yaan yancheng yangzhou yibin yichang yicheng
1    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
2    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
3    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
4    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
5    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
6    FALSE   FALSE   FALSE FALSE FALSE    FALSE    FALSE FALSE   FALSE   FALSE
  yichun yingtan yiyang yueyang zhenjiang zhoushan zhuzhou zigong ziyang
1  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
2  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
3  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
4  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
5  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
6  FALSE   FALSE  FALSE   FALSE     FALSE    FALSE   FALSE  FALSE  FALSE
```
Then use this data to fit the model
```
set.seed(1234)
rf_final<-randomForest(Carbon.dioxide.emissions~.,data=city_features,
                       mtry=60,maxnodes=50,ntree=500,importance=TRUE)
rf_final

Call:
 randomForest(formula = Carbon.dioxide.emissions ~ ., data = city_features,      mtry = 60, maxnodes = 50, ntree = 500, importance = TRUE) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 60

          Mean of squared residuals: 1.127609
                    % Var explained: -22.02
```

Work!
